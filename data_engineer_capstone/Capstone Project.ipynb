{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "this project aims to create an ETL pipeline for I94 immigration, global land temperatures and US demographics datasets to form an analytics database on immigration events.\n",
    "A use case for this analytics is to find immigration patterns to the US. For example, we can find answears to questions such as,\n",
    "What kind of people contries are they have warmer or colder who has large numbers immigrate to US?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, dayofmonth, dayofweek, month, year, weekofyear\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "import utilities\n",
    "import etl_functions\n",
    "\n",
    "import importlib\n",
    "importlib.reload(utilities)\n",
    "from utilities import visualize_missing_values, clean_immigration_data, clean_temperature_data\n",
    "from utilities import clean_demographics_data, print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Loading Configuration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 1: Project scope and Gathering Data\n",
    "\n",
    "## Project Scope \n",
    "<hr></hr>\n",
    "\n",
    "To create the analytics database, the following steps will be carried out:\n",
    "* Using Spark to load the data into dataframes.\n",
    "* Exploratory data analysis of I94 immigration dataset to identify missing values and strategies and data cleaning.\n",
    "* Exploratory data analysis of demographics dataset to identify missing values and strategies and data cleaning.\n",
    "* Exploratory data analysis of global land temperatures by city dataset to identify missing values and strategies for data cleaning.\n",
    "* Create dimension tables.\n",
    "    * Create immigration calendar dimension table from I94 immigration dataset, linked to fact table with arrdate field.\n",
    "    * Create country dimension table from the I94 immigration and the global temperatures dataset. The global land temperatures data was aggregated at country level. The table links to the fact table through the country of residence code allowing analysts to understand correlation between country of residence climate and immigration to US states. \n",
    "    * Create usa demographics dimension table from the us cities demographics data. This table links to the fact table through the state code field. \n",
    "    \n",
    "* Create fact table from the clean I94 immigration dataset and the visa_type dimension.\n",
    "\n",
    "<p>\n",
    "The technology stack used in this project is <b>Amazon S3, Apache Spark </b>.\n",
    "</p>\n",
    "\n",
    "While the whole project has been implemented on this notebook, provisions has been made to run the ETL on a spark cluster through etl.py. The etl.py script reads data from S3 and creates fact and dimesion tables through Spark that are loaded back into S3.\n",
    "\n",
    "## Data Descriptions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data: Data Description \n",
    "<hr style=\"background-color: #b7d0e2;\"/> \n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. In the past all foreign visitors to the U.S. arriving via air or sea were required to complete paper Customs and Border Protection Form I-94 Arrival/Departure Record or Form I-94W Nonimmigrant Visa Waiver Arrival/Departure Record and this dataset comes from this forms. \n",
    "\n",
    "This dataset forms the core of the data warehouse and the customer repository has a years worth of data for the year 2016 and the dataset is divided by month. For this project the data is in a folder located at ../../data/18-83510-I94-Data-2016/. Each months data is stored in an SAS binary database storage format <i>sas7bdat</i>. For this project we have chosen going to work with data for the month of April. However, the data extraction, transformation and loading utilities functions have been designed to work with any month's worth of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Loading I94 Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration_df =spark.read.format('com.github.saurfang.sas.spark').load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# display the first five records\n",
    "immigration_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# counts the total number of records\n",
    "print(immigration_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.select(\"visapost\").dropDuplicates().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Immigration Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">cicid</td><td class=\"tg-0pky\">Unique record ID</td>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td>\n",
    " <tr><td class=\"tg-0pky\">i94cit</td><td class=\"tg-0pky\">3 digit code for immigrant country of birth</td>\n",
    " <tr><td class=\"tg-0pky\">i94res</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td>\n",
    " <tr><td class=\"tg-0pky\">insnum</td><td class=\"tg-0pky\">INS number</td>\n",
    " <tr><td class=\"tg-0pky\">airline</td><td class=\"tg-0pky\">Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">admnum</td><td class=\"tg-0pky\">Admission Number</td>\n",
    " <tr><td class=\"tg-0pky\">fltno</td><td class=\"tg-0pky\">Flight number of Airline used to arrive in U.S.</td>\n",
    " <tr><td class=\"tg-0pky\">visatype</td><td class=\"tg-0pky\">Class of admission legally admitting the non-immigrant to temporarily stay in U.S.</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data: Data Description \n",
    "\n",
    "<hr style=\"background-color: #b7d0e2;\"/> \n",
    "The World Temperature dataset represents global land temperatures by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Load World Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = '../../data_samples/GlobalLandTempretureByCity_sample.csv'\n",
    "temperature_df = spark.read.csv(file_name, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the first five records\n",
    "temperature_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">dt</td><td class=\"tg-0pky\">Date</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperature</td><td class=\"tg-0pky\">Global average land temperature in celsius</td>\n",
    " <tr><td class=\"tg-0pky\">AverageTemperatureUncertainty</td><td class=\"tg-0pky\">95% confidence interval around the average</td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">Name of City</td>\n",
    " <tr><td class=\"tg-0pky\">Country</td><td class=\"tg-0pky\">Name of Country</td>\n",
    " <tr><td class=\"tg-0pky\">Latitude</td><td class=\"tg-0pky\">City Latitude</td>\n",
    " <tr><td class=\"tg-0pky\">Longitude</td><td class=\"tg-0pky\">City Longitude</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The total number of records\n",
    "print(temperature_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data: Data Description \n",
    "<hr style=\"background-color: #b7d0e2;\"/> \n",
    "\n",
    "This data contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_name = \"../../data_samples/us-cities-demographics.csv\"\n",
    "demographics_df = spark.read.csv(file_name, inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The first five records\n",
    "demographics_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<b><i>Data dictionary</i></b>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">State Code</td><td class=\"tg-0pky\">Code of the US state</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the total number of records\n",
    "print(demographics_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Exploratory Data Analysis: Immigration data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# list all files in the customer repository\n",
    "files = os.listdir('../../data/18-83510-I94-Data-2016/')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lets see the dataframe schema\n",
    "immigration_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Visualize Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Visualize missing values\n",
    "utilities.visualize_missing_values_spark(immigration_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Cleaning steps: \n",
    "\n",
    "* Dropping all columns missing values.Assumption (SLA is 90%)\n",
    "* Dropping all rows contain missing values. It's a NAN row so we don't need to consider it.Assumption (SLA is 100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Drop columns with significant missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# columns have over 90% missing values\n",
    "cols = ['occup', 'entdepu','insnum']\n",
    "\n",
    "# drop columns have NAN \n",
    "immigration_df_dropped_cols = immigration_df.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# display the new schema\n",
    "immigration_df_dropped_cols.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "immigration_df_dropped_cols = immigration_df_dropped_cols.dropDuplicates(['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get a count after dropping duplicates\n",
    "print(immigration_df_dropped_cols.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Data has no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "immigration_df_dropped_cols = immigration_df_dropped_cols.dropna(how='all', subset=['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get a count after dropping rows with missing values\n",
    "print(immigration_df_dropped_cols.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "> Immigration data has no rows with missing record ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean the immigration dataframe\n",
    "immigration_cleansed_df = utilities.clean_spark_immigration_data(immigration_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exploratory Data Analysis: World Temperature Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print dataframe schema\n",
    "temperature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Visualize Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visualize missing values by columns \n",
    "temperature_df_new = temperature_df.withColumn(\"dt\",col(\"dt\").cast(StringType())) # convert dt column dataType to string\n",
    "utilities.visualize_missing_values_spark(temperature_df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Cleaning Steps\n",
    "\n",
    "-  Drop all rows with missing average temperature\n",
    "-  Drop duplicate columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean the data\n",
    "new_temperature_df = utilities.clean_spark_temperature_data(temperature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exploratory Data Analysis: U.S. City Demographic Data \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# count the number of records in dataset\n",
    "print(demographics_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print database schema\n",
    "demographics_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Visualize Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "utilities.visualize_missing_values_spark(demographics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# View columns with missing data\n",
    "df_with_nulls = pd.DataFrame(data= demographics_df.toPandas().isnull().sum(), columns=['values'])\n",
    "df_with_nulls = df_with_nulls.reset_index()\n",
    "df_with_nulls.columns = ['cols', 'values']\n",
    "\n",
    "# calculate % missing values\n",
    "df_with_nulls['% missing values'] = 100*df_with_nulls['values']/demographics_df.count()\n",
    "df_with_nulls[df_with_nulls['% missing values']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Cleaning Steps\n",
    "\n",
    "-  demographics dataset has very few missing values\n",
    "-  Drop duplicate columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean demographics data\n",
    "demographics_final_df = utilities.clean_spark_demographics_data(demographics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 3: Defining the Data Model\n",
    "#### 3.1 Data Model (Star schema)\n",
    "\n",
    "![Database schema](images/data_model.png)\n",
    "\n",
    "The country dimension table is made up of data from the global land temperatures by city and the immigration datasets. The combination of these two datasets allows analysts to study correlations between global land temperatures and immigration patterns to the US.\n",
    "\n",
    "The us demographics dimension table comes from the demographics dataset and linked to the immigration fact table at US state level. This dimension would allow analysts to get insights and KPIs into migration patterns into the US based on demographics as well as overall population of states. We could ask questions such as, do populous states attracting more visitors on a monthly basis? One envisions a dashboard that could be designed based on the data model with drill downs into gradular information on visits to the US. Such a dashboard could foster a culture of data driven decision making within tourism and immigration departments at state level. \n",
    "\n",
    "The visa type dimension table comes from the immigration datasets and links to the immigaration by the  visa_type_key. \n",
    "\n",
    "The immigration fact table data comes from the immigration data sets and contains keys that links to the dimension tables. The data dictionary of the immigration dataset contains detailed information on the data that makes up the fact table. \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are as follows:\n",
    "\n",
    "* 1- Loading the datasets \n",
    "* 2- Cleaning the I94 Immigration data to create Spark dataframe for each month\n",
    "* 3- creating visa_type dimension table\n",
    "* 4- creating calendar dimension table\n",
    "* Extract clean global temperatures data\n",
    "* Create country dimension table\n",
    "* Create immigration fact table\n",
    "* Load demographics data\n",
    "* Clean demographics data\n",
    "* Create demographic dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the immigration calendar dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_calendar_dim(df, output_data):\n",
    "    \"\"\"This function creates an immigration calendar based on arrival date key\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # Convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # create initial calendar df from arrdate column\n",
    "    calendar_df = df.select(['arrdate']).withColumn(\"arrdate\", get_datetime(df.arrdate)).distinct()\n",
    "    \n",
    "    # expand df by adding other calendar columns\n",
    "    calendar_df = calendar_df.withColumn('arrival_day', dayofmonth('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_week', weekofyear('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_month', month('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_year', year('arrdate'))\n",
    "    calendar_df = calendar_df.withColumn('arrival_weekday', dayofweek('arrdate'))\n",
    "\n",
    "    # create an id field in calendar df\n",
    "    calendar_df = calendar_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write the calendar dimension to parquet file\n",
    "    partition_columns = ['arrival_year', 'arrival_month', 'arrival_week']\n",
    "    calendar_df.write.parquet(output_data + \"immigration_calendar\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "    \n",
    "    return calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"tables/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calendar_df = create_immigration_calendar_dim(immigration_cleansed_df, output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the country dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_country_dim_table(df, temp_df, output_data):\n",
    "    \"\"\"This function creates a country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :temp_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get the aggregated temperature data\n",
    "    agg_temp = utilities.aggregate_temperature_data(temp_df).toPandas()\n",
    "    # load the i94res to country mapping data\n",
    "    mapping_codes = pd.read_csv('i94res.csv')\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_country_average_temperature(name):\n",
    "        print(\"Processing: \", name)\n",
    "        avg_temp = agg_temp[agg_temp['Country']==name]['average_temperature']\n",
    "        \n",
    "        if not avg_temp.empty:\n",
    "            return str(avg_temp.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @udf()\n",
    "    def get_country_name(code):\n",
    "        name = mapping_codes[mapping_codes['code']==code]['Name'].iloc[0]\n",
    "        \n",
    "        if name:\n",
    "            return name.title()\n",
    "        return None\n",
    "        \n",
    "    # select and rename i94res column\n",
    "    dim_df = df.select(['i94res']).distinct() \\\n",
    "                .withColumnRenamed('i94res', 'country_code')\n",
    "    \n",
    "    # create country_name column\n",
    "    dim_df = dim_df.withColumn('country_name', get_country_name(dim_df.country_code))\n",
    "    \n",
    "    # create average_temperature column\n",
    "    dim_df = dim_df.withColumn('average_temperature', get_country_average_temperature(dim_df.country_name))\n",
    "    \n",
    "    # write the dimension to a parquet file\n",
    "    dim_df.write.parquet(output_data + \"country\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_f = create_country_dim_table(immigration_cleansed_df, new_temperature_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_dim_f.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the visa type dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_visa_type_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a visa type dimension from the immigration data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # create visatype df from visatype column\n",
    "    visatype_df = df.select(['visatype']).distinct()\n",
    "    \n",
    "    # add an id column\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    visatype_df.write.parquet(output_data + \"visatype\", mode=\"overwrite\")\n",
    "    \n",
    "    return visatype_df\n",
    "\n",
    "def get_visa_type_dimension(output_data):\n",
    "    return spark.read.parquet(output_data + \"visatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test create visa_type dimension function\n",
    "visatype_df = create_visa_type_dimension_table(immigration_cleansed_df, output_data)\n",
    "visatype_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the demographics dimension table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_demographics_dimension_table(df, output_data):\n",
    "    \"\"\"This function creates a us demographics dimension table from the us cities demographics data.\n",
    "    \n",
    "    :param df: spark dataframe of us demographics survey data\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing demographics dimension\n",
    "    \"\"\"\n",
    "    dim_df = df.withColumnRenamed('Median Age','median_age') \\\n",
    "            .withColumnRenamed('Male Population', 'male_population') \\\n",
    "            .withColumnRenamed('Female Population', 'female_population') \\\n",
    "            .withColumnRenamed('Total Population', 'total_population') \\\n",
    "            .withColumnRenamed('Number of Veterans', 'number_of_veterans') \\\n",
    "            .withColumnRenamed('Foreign-born', 'foreign_born') \\\n",
    "            .withColumnRenamed('Average Household Size', 'average_household_size') \\\n",
    "            .withColumnRenamed('State Code', 'state_code')\n",
    "    # lets add an id column\n",
    "    dim_df = dim_df.withColumn('id', monotonically_increasing_id())\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    dim_df.write.parquet(output_data + \"demographics\", mode=\"overwrite\")\n",
    "    \n",
    "    return dim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics_dim_df = create_demographics_dimension_table(demographics_final_df, output_data)\n",
    "demographics_dim_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create the immigration fact table\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigration_fact_table(df, output_data):\n",
    "    \"\"\"This function creates an country dimension from the immigration and global land temperatures data.\n",
    "    \n",
    "    :param df: spark dataframe of immigration events\n",
    "    :param visa_type_df: spark dataframe of global land temperatures data.\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing calendar dimension\n",
    "    \"\"\"\n",
    "    # get visa_type dimension\n",
    "    dim_df = get_visa_type_dimension(output_data).toPandas()\n",
    "    \n",
    "    @udf('string')\n",
    "    def get_visa_key(visa_type):\n",
    "        \"\"\"user defined function to get visa key\n",
    "        \n",
    "        :param visa_type: US non-immigrant visa type\n",
    "        :return: corresponding visa key\n",
    "        \"\"\"\n",
    "        key_series = dim_df[dim_df['visatype']==visa_type]['visa_type_key']\n",
    "        \n",
    "        if not key_series.empty:\n",
    "            return str(key_series.iloc[0])\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # create a udf to convert arrival date in SAS format to datetime object\n",
    "    get_datetime = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "    \n",
    "    # rename columns to align with data model\n",
    "    df = df.withColumnRenamed('cicid','record_id') \\\n",
    "            .withColumnRenamed('i94res', 'country_residence_code') \\\n",
    "            .withColumnRenamed('i94addr', 'state_code') \n",
    "    \n",
    "    # create visa_type key\n",
    "    df = df.withColumn('visa_type_key', get_visa_key('visatype'))\n",
    "    \n",
    "    # convert arrival date into datetime object\n",
    "    df = df.withColumn(\"arrdate\", get_datetime(df.arrdate))\n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    df.write.parquet(output_data + \"immigration_fact\", mode=\"overwrite\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fact_df = create_immigration_fact_table(immigration_cleansed_df, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    # load data\n",
    "    \n",
    "    # run cleaning functions\n",
    "    \n",
    "    # create fact and dimension tables\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "The data quality checks ensures that the ETL has created fact and dimension tables with adequate records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_dfs = {\n",
    "    'immigration_fact': immigration_fact_df,\n",
    "    'visa_type_dim': visatype_df,\n",
    "    'calendar_dim': calendar_df,\n",
    "    'usa_demographics_dim': demographics_dim_df,\n",
    "    'country_dim': country_dim_f\n",
    "}\n",
    "for table_name, table_df in table_dfs.items():\n",
    "    # quality check for table\n",
    "    etl_functions.quality_checks(table_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### Fact Table - data dictionary\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">record_id</td><td class=\"tg-0pky\">Unique record ID</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_residence_code</td><td class=\"tg-0pky\">3 digit code for immigrant country of residence </td></tr>    \n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">A numerical key that links to the visa_type dimension table</td></tr>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94yr</td><td class=\"tg-0pky\">4 digit year</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mon</td><td class=\"tg-0pky\">Numeric month</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94port</td><td class=\"tg-0pky\">Port of admission</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival Date in the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94mode</td><td class=\"tg-0pky\">Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94addr</td><td class=\"tg-0pky\">USA State of arrival</td></tr>\n",
    " <tr><td class=\"tg-0pky\">depdate</td><td class=\"tg-0pky\">Departure Date from the USA</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94bir</td><td class=\"tg-0pky\">Age of Respondent in Years</td></tr>\n",
    " <tr><td class=\"tg-0pky\">i94visa</td><td class=\"tg-0pky\">Visa codes collapsed into three categories</td></tr>\n",
    " <tr><td class=\"tg-0pky\">count</td><td class=\"tg-0pky\">Field used for summary statistics</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtadfile</td><td class=\"tg-0pky\">Character Date Field - Date added to I-94 Files</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visapost</td><td class=\"tg-0pky\">Department of State where where Visa was issued </td></tr>\n",
    " <tr><td class=\"tg-0pky\">occup</td><td class=\"tg-0pky\">Occupation that will be performed in U.S</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepa</td><td class=\"tg-0pky\">Arrival Flag - admitted or paroled into the U.S.</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepd</td><td class=\"tg-0pky\">Departure Flag - Departed, lost I-94 or is deceased</td></tr>\n",
    " <tr><td class=\"tg-0pky\">entdepu</td><td class=\"tg-0pky\">Update Flag - Either apprehended, overstayed, adjusted to perm residence</td></tr>\n",
    " <tr><td class=\"tg-0pky\">matflag</td><td class=\"tg-0pky\">Match flag - Match of arrival and departure records</td></tr>\n",
    " <tr><td class=\"tg-0pky\">biryear</td><td class=\"tg-0pky\">4 digit year of birth</td></tr>\n",
    " <tr><td class=\"tg-0pky\">dtaddto</td><td class=\"tg-0pky\">Character Date Field - Date to which admitted to U.S. (allowed to stay until)</td></tr>\n",
    " <tr><td class=\"tg-0pky\">gender</td><td class=\"tg-0pky\">Non-immigrant sex</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Country Dimension Table - data dictionary\n",
    "<p>  \n",
    "<i>The country code and country_name fields come from the labels description SAS file while the average_temperature data comes from the global land temperature by cities data.</i>\n",
    "</p>\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">country_code</td><td class=\"tg-0pky\">Unique country code</td></tr>\n",
    " <tr><td class=\"tg-0pky\">country_name</td><td class=\"tg-0pky\">Name of country</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">average_temperature</td><td class=\"tg-0pky\">Average temperature of country</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Visa Type Dimension Table - data dictionary\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type_key</td><td class=\"tg-0pky\">Unique id for each visa issued</td></tr>\n",
    " <tr><td class=\"tg-0pky\">visa_type</td><td class=\"tg-0pky\">Name of visa</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " ##### Immigration Calendar Dimension Table - data dictionary\n",
    "<p>\n",
    "<i>The whole of this dataset comes from the immigration dataset.</i>\n",
    "</p>\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Unique id</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrdate</td><td class=\"tg-0pky\">Arrival date into US</td></tr>    \n",
    " <tr><td class=\"tg-0pky\">arrival_year</td><td class=\"tg-0pky\">Arrival year into US</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_month</td><td class=\"tg-0pky\">Arrival MonthS</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_day</td><td class=\"tg-0pky\">Arrival Day</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_week</td><td class=\"tg-0pky\">Arrival Week</td></tr>\n",
    " <tr><td class=\"tg-0pky\">arrival_weekday</td><td class=\"tg-0pky\">Arrival WeekDay</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Demographics Dimension Table - data dictionary\n",
    "<p>\n",
    "<i>The whole of this dataset comes from the us cities demographics data.</i>\n",
    "</p>\n",
    "\n",
    "<table class=\"tg\" align=\"left\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Feature</th>\n",
    "    <th class=\"tg-0pky\">Description</th>\n",
    "  </tr>\n",
    " <tr><td class=\"tg-0pky\">id</td><td class=\"tg-0pky\">Record id</td>\n",
    " <tr><td class=\"tg-0pky\">state_code</td><td class=\"tg-0pky\">US state code </td>\n",
    " <tr><td class=\"tg-0pky\">City</td><td class=\"tg-0pky\">City Name</td>\n",
    " <tr><td class=\"tg-0pky\">State</td><td class=\"tg-0pky\">US State where city is located</td>\n",
    " <tr><td class=\"tg-0pky\">Median Age</td><td class=\"tg-0pky\">Median age of the population</td>\n",
    " <tr><td class=\"tg-0pky\">Male Population</td><td class=\"tg-0pky\">Count of male population</td>\n",
    " <tr><td class=\"tg-0pky\">Female Population</td><td class=\"tg-0pky\">Count of female population</td>\n",
    " <tr><td class=\"tg-0pky\">Total Population</td><td class=\"tg-0pky\">Count of total population</td>\n",
    " <tr><td class=\"tg-0pky\">Number of Veterans</td><td class=\"tg-0pky\">Count of total Veterans</td>\n",
    " <tr><td class=\"tg-0pky\">Foreign born</td><td class=\"tg-0pky\">Count of residents of the city that were not born in the city</td>\n",
    " <tr><td class=\"tg-0pky\">Average Household Size</td><td class=\"tg-0pky\">Average city household size</td>\n",
    " <tr><td class=\"tg-0pky\">Race</td><td class=\"tg-0pky\">Respondent race</td>\n",
    " <tr><td class=\"tg-0pky\">Count</td><td class=\"tg-0pky\">Count of city's individual per race</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project\n",
    "    * Apache spark was used because of:\n",
    "        * it's ability to handle multiple file formats with large amounts of data. \n",
    "        * Apache Spark offers a lightning-fast unified analytics engine for big data.\n",
    "        * Spark has easy-to-use APIs for operating on large datasets\n",
    "* Propose how often the data should be updated and why.\n",
    "    * The current I94 immigration data is updated monthly, and hence the data will be updated monthly. \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * Spark can handle the increase but we would consider increasing the number of nodes in our cluster.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     * In this scenario, Apache Airflow will be used to schedule and run data pipelines.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * In this scenario, we would move our analytics database into Amazon Redshift. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
